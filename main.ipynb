{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccdd05ae",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0dba7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import autogen\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "808e2a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL WARNING: Cannot import limit_context_window. Context filtering is DISABLED.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: API Configuration, Model Strategy, and Logging Logic\n",
    "\n",
    "# La funzione per la limitazione del contesto deve essere importata esplicitamente da client_utils.\n",
    "try:\n",
    "    from autogen.oai.client_utils import limit_context_window\n",
    "except ImportError:\n",
    "    try:\n",
    "        from autogen.oai.utils import limit_context_window\n",
    "    except ImportError:\n",
    "        print(\"CRITICAL WARNING: Cannot import limit_context_window. Context filtering is DISABLED.\")\n",
    "        def limit_context_window(messages, max_tokens):\n",
    "            return messages, None\n",
    "        \n",
    "# --- Carica Variabili da .env ---\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. API Key Management ---\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    raise ValueError(\"GROQ_API_KEY is required but not found in environment.\")\n",
    "\n",
    "GROQ_BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "# --- 2. Model Leaderboard (Fallback Strategy) ---\n",
    "config_list = [\n",
    "    {\"model\": \"openai/gpt-oss-120b\", \"api_key\": os.environ.get(\"GROQ_API_KEY\"), \"base_url\": GROQ_BASE_URL},\n",
    "    {\"model\": \"llama-3.3-70b-versatile\", \"api_key\": os.environ.get(\"GROQ_API_KEY\"), \"base_url\": GROQ_BASE_URL},\n",
    "    {\"model\": \"mixtral-8x7b-32768\", \"api_key\": os.environ.get(\"GROQ_API_KEY\"), \"base_url\": GROQ_BASE_URL},\n",
    "    {\"model\": \"llama-3.1-8b-instant\", \"api_key\": os.environ.get(\"GROQ_API_KEY\"), \"base_url\": GROQ_BASE_URL}\n",
    "]\n",
    "\n",
    "LLM_MODEL_NAME = config_list[0][\"model\"]\n",
    "\n",
    "# DEFINIAMO IL LIMITE DI TOKEN UNA SOLA VOLTA\n",
    "MAX_CONTEXT_TOKENS = 2500\n",
    "\n",
    "# CONFIGURAZIONE LLM\n",
    "llm_config = {\n",
    "    \"timeout\": 120,\n",
    "    \"config_list\": config_list,\n",
    "    \"cache_seed\": 42,\n",
    "    \"max_tokens\": MAX_CONTEXT_TOKENS,  # Spostato qui dal extra_kwargs\n",
    "}\n",
    "\n",
    "# --- 3. Robust Python Logging Function ---\n",
    "def save_chat_history_to_txt(chat_manager, filename_prefix=\"execution_log\"):\n",
    "    \"\"\"\n",
    "    Extracts chat history and saves it to a clean, readable TXT file.\n",
    "    Deterministic Python code, no agents involved.\n",
    "    \"\"\"\n",
    "    log_dir = \"logs\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = os.path.join(log_dir, f\"{filename_prefix}_{timestamp}.txt\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"=== PROJECT FORECASTING LOG - {timestamp} ===\\n\")\n",
    "            f.write(f\"Model Strategy: Fallback Order {[c['model'] for c in config_list]}\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\\n\")\n",
    "            \n",
    "            messages = list(chat_manager.chat_messages.values())[-1]\n",
    "            \n",
    "            for message in messages:\n",
    "                sender = message.get('name', message.get('role', 'Unknown'))\n",
    "                content = message.get('content', '')\n",
    "                \n",
    "                f.write(f\"[{datetime.datetime.now().strftime('%H:%M:%S')}] --- SENDER: {sender.upper()} ---\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                \n",
    "                if \"function_call\" in message and message[\"function_call\"]:\n",
    "                     f.write(f\">> TOOL CALL: {message['function_call'].get('name')}\\n\")\n",
    "                     f.write(f\">> ARGS: {message['function_call'].get('arguments')}\\n\")\n",
    "                \n",
    "                f.write(content)\n",
    "                f.write(\"\\n\\n\" + \"=\"*60 + \"\\n\\n\")\n",
    "                \n",
    "        print(f\"✅ Conversation log successfully saved to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving log: {e}\")\n",
    "\n",
    "def create_summary_of_messages(messages):\n",
    "    \"\"\"\n",
    "    Crea un riassunto intelligente dei messaggi mantenendo il contesto critico.\n",
    "    Mantiene: system messages + riassunto messaggi vecchi + ultimi 5 messaggi completi.\n",
    "    \"\"\"\n",
    "    if len(messages) <= 8:  # Se pochi messaggi, non serve riassunto\n",
    "        return messages\n",
    "    \n",
    "    # Separa system messages dai messaggi normali\n",
    "    system_msgs = [m for m in messages if m.get(\"role\") == \"system\"]\n",
    "    other_msgs = [m for m in messages if m.get(\"role\") != \"system\"]\n",
    "    \n",
    "    if len(other_msgs) <= 5:\n",
    "        return messages\n",
    "    \n",
    "    # Messaggi da riassumere (tutti tranne gli ultimi 5)\n",
    "    to_summarize = other_msgs[:-5]\n",
    "    recent_msgs = other_msgs[-5:]\n",
    "    \n",
    "    # Crea riassunto testuale\n",
    "    summary_text = \"=== CONVERSATION SUMMARY ===\\n\"\n",
    "    for msg in to_summarize:\n",
    "        role = msg.get(\"role\", \"unknown\")\n",
    "        name = msg.get(\"name\", role)\n",
    "        content = msg.get(\"content\", \"\")[:150]  # Max 150 char per messaggio\n",
    "        summary_text += f\"[{name}]: {content}...\\n\"\n",
    "    \n",
    "    summary_text += \"=== END SUMMARY ===\\n\"\n",
    "    \n",
    "    # Crea messaggio di riassunto\n",
    "    summary_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": summary_text\n",
    "    }\n",
    "    \n",
    "    # Ritorna: system messages + summary + ultimi 5 messaggi\n",
    "    return system_msgs + [summary_message] + recent_msgs\n",
    "\n",
    "def smart_message_transform(messages):\n",
    "    \"\"\"\n",
    "    Trasforma i messaggi per ridurre i token mantenendo il contesto.\n",
    "    Conta token approssimativi e riassume se necessario.\n",
    "    \"\"\"\n",
    "    # Conta token approssimativi (4 caratteri = 1 token circa)\n",
    "    total_chars = sum(len(str(m.get(\"content\", \"\"))) for m in messages)\n",
    "    estimated_tokens = total_chars / 4\n",
    "    \n",
    "    # Se siamo sopra ~3500 token stimati, riassumi\n",
    "    if estimated_tokens > 3500:\n",
    "        print(f\"⚠️ Token Warning: ~{int(estimated_tokens)} tokens detected. Summarizing old messages...\")\n",
    "        return create_summary_of_messages(messages)\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb845286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Global Variables and Admin Agent Setup\n",
    "\n",
    "# --- Global Configuration Variables ---\n",
    "# Safety and Control - MANTIENI I TUOI VALORI\n",
    "ASK_CODE_PERMISSION = True   # ✅ MANTIENI: Chiedi conferma\n",
    "CAN_ASK_HUMAN = True          # ✅ MANTIENI: Puoi intervenire\n",
    "\n",
    "# Trading Strategy Parameters\n",
    "TRADING_COMMISSION_PCT = 0.00055\n",
    "SLIPPAGE_K = 0.01\n",
    "INITIAL_CAPITAL_USDT = 10000.0\n",
    "\n",
    "# Time Constraints (UTC)\n",
    "OPEN_TIME_UTC_START = 11\n",
    "OPEN_TIME_UTC_END = 19\n",
    "MAX_PREDICTION_WINDOW_MINUTES = 23 * 60 + 59\n",
    "\n",
    "# --- Admin Agent Definition ---\n",
    "admin = autogen.UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    system_message=\"Execute code after user approval.\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \".\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    "    max_consecutive_auto_reply=999,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a949e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_assistant_agent(name, role_description):\n",
    "    \"\"\"\n",
    "    Crea un AssistantAgent ottimizzato per ESEGUIRE codice, non solo discuterne.\n",
    "    \"\"\"\n",
    "    common_instruction = (\n",
    "        f\"Using Groq API ({LLM_MODEL_NAME}). \"\n",
    "        \"CRITICAL: Write ```python code blocks that Admin can execute immediately. \"\n",
    "        \"DO NOT just discuss code - PROPOSE it for execution. \"\n",
    "        \"BE CONCISE: max 3 sentences + code block. \"\n",
    "        \"Use CPU-only models (RandomForest/XGBoost). \"\n",
    "    )\n",
    "    \n",
    "    # Funzione per terminare su messaggi vuoti (evita loop)\n",
    "    def is_termination_msg(content):\n",
    "        if content is None:\n",
    "            return True\n",
    "        msg_content = content.get(\"content\", \"\")\n",
    "        if msg_content.strip() == \"\":\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    return autogen.AssistantAgent(\n",
    "        name=name,\n",
    "        llm_config=llm_config,\n",
    "        system_message=f\"{role_description} {common_instruction}\",\n",
    "        is_termination_msg=is_termination_msg,\n",
    "        max_consecutive_auto_reply=2,\n",
    "    )\n",
    "\n",
    "# --- Define the Specialists ---\n",
    "architect = create_assistant_agent(\n",
    "    \"Project_Architect\", \n",
    "    \"You are an expert in robust project structuring. Your first task is to create the folder structure (src, data/raw, data/processed, models, testing) and a comprehensive config.py file based on all initial parameters.\"\n",
    ")\n",
    "\n",
    "data_scraper = create_assistant_agent(\n",
    "    \"Data_Scraper\", \n",
    "    \"You are the world's expert in data cleaning and integration. Your task is to **load ALL local CSV files** from the 'data/raw' folder, perform cleaning (NaN, missing data), time-series synchronization (using the 'timestamp' column), and feature generation, preparing a single clean DataFrame for the Model_Builder. **DO NOT fetch data from the web.**\"\n",
    ")\n",
    "\n",
    "model_builder = create_assistant_agent(\n",
    "    \"Model_Builder\", \n",
    "    \"You are the top Data Scientist for time series. Perform feature engineering, train a robust **CPU-compatible model (e.g., RandomForest, XGBoost, or simple Scikit-learn models)** to predict maximum price change, and save the final model.\"\n",
    ")\n",
    "\n",
    "optimizer = create_assistant_agent(\n",
    "    \"Optimizer_Simulator\", \n",
    "    \"You are the Financial Risk and Optimization expert. Create the trading simulation (100/L - 0.5% liquidation logic) and optimize Integer Leverage (1-100), Stop Loss, and Take Profit to maximize log_growth.\"\n",
    ")\n",
    "\n",
    "code_tester = create_assistant_agent(\n",
    "    \"Code_Tester\", \n",
    "    \"You are the Production QA Engineer. Stress-test the code, ensure robust NaN error handling, and provide the final inference code formatted as requested.\"\n",
    ")\n",
    "\n",
    "# --- Custom Manager con Message Transform ---\n",
    "class SummarizingGroupChatManager(autogen.GroupChatManager):\n",
    "    \"\"\"\n",
    "    GroupChatManager che riassume automaticamente i messaggi lunghi.\n",
    "    \"\"\"\n",
    "    def _process_received_message(self, message, sender, silent):\n",
    "        # Prima di processare, applica il transform\n",
    "        if hasattr(self, 'groupchat') and hasattr(self.groupchat, 'messages'):\n",
    "            self.groupchat.messages = smart_message_transform(self.groupchat.messages)\n",
    "        \n",
    "        # Chiama il metodo originale\n",
    "        return super()._process_received_message(message, sender, silent)\n",
    "\n",
    "# --- Group Chat Setup CON TUTTI GLI AGENTI ---\n",
    "agent_list = [admin, architect, data_scraper, model_builder, optimizer, code_tester]\n",
    "\n",
    "group_chat = autogen.GroupChat(\n",
    "    agents=agent_list, \n",
    "    messages=[], \n",
    "    max_round=30,  # ⚠️ RIDOTTO da 50 a 30 per evitare loop troppo lunghi\n",
    "    speaker_selection_method=\"auto\",\n",
    ")\n",
    "\n",
    "# Usa il manager custom con summarization\n",
    "manager = SummarizingGroupChatManager(\n",
    "    groupchat=group_chat, \n",
    "    llm_config=llm_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f88b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ESECUZIONE SEQUENZIALE CON CONFERMA UTENTE\n",
      "============================================================\n",
      "\n",
      "[FASE 1/5] Setup Progetto\n",
      "------------------------------------------------------------\n",
      "\u001b[33mAdmin\u001b[0m (to Project_Architect):\n",
      "\n",
      "Create folders: src, data/raw, data/processed, models, testing. Create src/config.py that loads GROQ_API_KEY with os.getenv(). Write EXECUTABLE Python code in ```python blocks.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 12-12 14:46:28] {732} WARNING - Model openai/gpt-oss-120b is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mProject_Architect\u001b[0m (to Admin):\n",
      "\n",
      "Here’s a single script you can run now; it creates the required folder tree and writes a `src/config.py` that pulls the `GROQ_API_KEY` from the environment.\n",
      "\n",
      "```python\n",
      "import os\n",
      "from pathlib import Path\n",
      "\n",
      "# 1️⃣ Define the folder layout\n",
      "folders = [\n",
      "    Path(\"src\"),\n",
      "    Path(\"data/raw\"),\n",
      "    Path(\"data/processed\"),\n",
      "    Path(\"models\"),\n",
      "    Path(\"testing\"),\n",
      "]\n",
      "\n",
      "# 2️⃣ Create all missing directories (parents=True handles nested paths)\n",
      "for f in folders:\n",
      "    f.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "# 3️⃣ Write src/config.py with a safe env‑var loader\n",
      "config_path = Path(\"src/config.py\")\n",
      "config_path.write_text(\n",
      "    \"\"\"import os\n",
      "\n",
      "# Load the Groq API key from the environment; raise a clear error if missing\n",
      "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
      "if not GROQ_API_KEY:\n",
      "    raise EnvironmentError(\"GROQ_API_KEY not set in environment variables\")\n",
      "\"\"\"\n",
      ")\n",
      "\n",
      "print(\"✅ Folder structure created and src/config.py generated.\")\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to Project_Architect):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "âœ… Folder structure created and src/config.py generated.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 12-12 14:47:32] {732} WARNING - Model openai/gpt-oss-120b is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mProject_Architect\u001b[0m (to Admin):\n",
      "\n",
      "Below is a ready‑to‑run script that **re‑writes** `src/config.py` with a full, production‑ready configuration covering paths, model defaults, Groq API details, and a simple logging setup.\n",
      "\n",
      "```python\n",
      "import os\n",
      "from pathlib import Path\n",
      "import json\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 1️⃣ Define absolute project‑root paths (relative to this script)\n",
      "# ----------------------------------------------------------------------\n",
      "ROOT = Path(__file__).resolve().parent.parent  # project root (one level up)\n",
      "DATA_RAW = ROOT / \"data\" / \"raw\"\n",
      "DATA_PROC = ROOT / \"data\" / \"processed\"\n",
      "MODEL_DIR = ROOT / \"models\"\n",
      "SRC_DIR = ROOT / \"src\"\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 2️⃣ Build the config dictionary\n",
      "# ----------------------------------------------------------------------\n",
      "config = {\n",
      "    # ── Environment ─────────────────────\n",
      "    \"GROQ_API_KEY\": os.getenv(\"GROQ_API_KEY\"),\n",
      "    \"GROQ_API_URL\": \"https://api.groq.com/openai/v1\",\n",
      "\n",
      "    # ── Paths ────────────────────────────\n",
      "    \"PATHS\": {\n",
      "        \"ROOT\": str(ROOT),\n",
      "        \"RAW_DATA\": str(DATA_RAW),\n",
      "        \"PROCESSED_DATA\": str(DATA_PROC),\n",
      "        \"MODEL_DIR\": str(MODEL_DIR),\n",
      "        \"SRC_DIR\": str(SRC_DIR),\n",
      "    },\n",
      "\n",
      "    # ── Model defaults (CPU‑only) ────────\n",
      "    \"DEFAULT_MODEL\": \"random_forest\",   # or \"xgboost\"\n",
      "    \"RANDOM_FOREST_PARAMS\": {\n",
      "        \"n_estimators\": 200,\n",
      "        \"max_depth\": None,\n",
      "        \"random_state\": 42,\n",
      "        \"n_jobs\": -1,\n",
      "    },\n",
      "    \"XGBOOST_PARAMS\": {\n",
      "        \"n_estimators\": 200,\n",
      "        \"max_depth\": 6,\n",
      "        \"learning_rate\": 0.1,\n",
      "        \"objective\": \"reg:squarederror\",\n",
      "        \"n_jobs\": -1,\n",
      "        \"random_state\": 42,\n",
      "    },\n",
      "\n",
      "    # ── Logging ───────────────────────────\n",
      "    \"LOGGING\": {\n",
      "        \"level\": \"INFO\",\n",
      "        \"format\": \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
      "        \"datefmt\": \"%Y-%m-%d %H:%M:%S\",\n",
      "    },\n",
      "\n",
      "    # ── Misc ─────────────────────────────\n",
      "    \"RANDOM_SEED\": 42,\n",
      "    \"TEST_SPLIT\": 0.2,\n",
      "}\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 3️⃣ Validate critical env vars\n",
      "# ----------------------------------------------------------------------\n",
      "if not config[\"GROQ_API_KEY\"]:\n",
      "    raise EnvironmentError(\n",
      "        \"GROQ_API_KEY not found in environment variables. \"\n",
      "        \"Set it before running any Groq‑based code.\"\n",
      "    )\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 4️⃣ Write the config file (pretty‑printed JSON inside a Python module)\n",
      "# ----------------------------------------------------------------------\n",
      "config_path = SRC_DIR / \"config.py\"\n",
      "config_path.write_text(\n",
      "    f\"\"\"\\\"\\\"\\\"Auto‑generated project configuration.\n",
      "\n",
      "All paths are absolute; modify only if you relocate the project.\n",
      "\\\"\\\"\\\"\n",
      "\n",
      "import json\n",
      "from pathlib import Path\n",
      "\n",
      "# Environment\n",
      "GROQ_API_KEY = {json.dumps(config[\"GROQ_API_KEY\"])}\n",
      "GROQ_API_URL = \"{config[\"GROQ_API_URL\"]}\"\n",
      "\n",
      "# Paths\n",
      "PATHS = {json.dumps(config[\"PATHS\"], indent=4)}\n",
      "\n",
      "# Model defaults\n",
      "DEFAULT_MODEL = \"{config[\"DEFAULT_MODEL\"]}\"\n",
      "RANDOM_FOREST_PARAMS = {json.dumps(config[\"RANDOM_FOREST_PARAMS\"], indent=4)}\n",
      "XGBOOST_PARAMS = {json.dumps(config[\"XGBOOST_PARAMS\"], indent=4)}\n",
      "\n",
      "# Logging configuration\n",
      "LOGGING = {json.dumps(config[\"LOGGING\"], indent=4)}\n",
      "\n",
      "# Misc\n",
      "RANDOM_SEED = {config[\"RANDOM_SEED\"]}\n",
      "TEST_SPLIT = {config[\"TEST_SPLIT\"]}\n",
      "\n",
      "def as_dict():\n",
      "    \\\"\\\"\\\"Return the whole config as a plain dict (useful for debugging).\\\"\\\"\\\"\n",
      "    return {{\n",
      "        \"GROQ_API_KEY\": GROQ_API_KEY,\n",
      "        \"GROQ_API_URL\": GROQ_API_URL,\n",
      "        \"PATHS\": PATHS,\n",
      "        \"DEFAULT_MODEL\": DEFAULT_MODEL,\n",
      "        \"RANDOM_FOREST_PARAMS\": RANDOM_FOREST_PARAMS,\n",
      "        \"XGBOOST_PARAMS\": XGBOOST_PARAMS,\n",
      "        \"LOGGING\": LOGGING,\n",
      "        \"RANDOM_SEED\": RANDOM_SEED,\n",
      "        \"TEST_SPLIT\": TEST_SPLIT,\n",
      "    }}\n",
      "\"\"\"\n",
      ")\n",
      "\n",
      "print(f\"✅ Config written to {config_path}\")\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to Project_Architect):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 73, in <module>\n",
      "    config_path.write_text(\n",
      "  File \"C:\\Users\\Alessandro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pathlib.py\", line 1078, in write_text\n",
      "    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alessandro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pathlib.py\", line 1044, in open\n",
      "    return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Cartelle\\\\Mie\\\\Programmi personali in Python\\\\src\\\\config.py'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (1385b8b4-b0fd-4cd6-84ea-c826f85f39a9): Maximum number of consecutive auto-replies reached\u001b[0m\n",
      "\n",
      "[FASE 2/5] Creazione Target Variable\n",
      "------------------------------------------------------------\n",
      "\u001b[33mAdmin\u001b[0m (to Data_Scraper):\n",
      "\n",
      "CRITICAL TASK: Create the target variable from 'Bitcoin futures (USDT) 25-03-2020-10-36-00_07-12-2025-00-00-00 timeframe 1m.csv'. The target is: maximum absolute price change (for example +2000 USDT or -2500 USDT) from 11:00 UTC each day to 10:59 UTC of the day after. For each day: 1. Calculate: max_change = max(close) - min(close) for that window 2. Store as 'target_max_variation' column Save to data/processed/target_data.csv. Write EXECUTABLE Python code.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 12-12 14:56:15] {732} WARNING - Model openai/gpt-oss-120b is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mData_Scraper\u001b[0m (to Admin):\n",
      "\n",
      "Here is a short, ready‑to‑run script that creates the daily “max absolute price change” target from the specified 1‑minute Bitcoin futures file and saves it to **data/processed/target_data.csv**.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from pathlib import Path\n",
      "\n",
      "# 1️⃣ Load the raw 1‑minute futures data\n",
      "raw_path = Path(\"data/raw/Bitcoin futures (USDT) 25-03-2020-10-36-00_07-12-2025-00-00-00 timeframe 1m.csv\")\n",
      "df = pd.read_csv(raw_path, parse_dates=['timestamp'])\n",
      "\n",
      "# 2️⃣ Build a custom day that starts at 11:00 UTC (shift timestamps back by 11 h, then use the calendar date)\n",
      "df['day'] = (df['timestamp'] - pd.Timedelta(hours=11)).dt.floor('D')\n",
      "\n",
      "# 3️⃣ Compute max‑min close price per custom day (covers 11:00 → 10:59 next day)\n",
      "target = (\n",
      "    df.groupby('day')['close']\n",
      "      .agg(max_change=lambda x: x.max() - x.min())\n",
      "      .reset_index()\n",
      "      .rename(columns={'max_change': 'target_max_variation'})\n",
      ")\n",
      "\n",
      "# 4️⃣ Save the result\n",
      "out_path = Path(\"data/processed/target_data.csv\")\n",
      "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
      "target.to_csv(out_path, index=False)\n",
      "\n",
      "print(f\"Target file written to {out_path}\")\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to Data_Scraper):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 13, in <module>\n",
      "    df.groupby('day')['close']\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^\n",
      "  File \"c:\\Users\\Alessandro\\Desktop\\Cartelle\\Mie\\Programmi personali in Python\\Forecasting crypto with agents\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\", line 1951, in __getitem__\n",
      "    return super().__getitem__(key)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Alessandro\\Desktop\\Cartelle\\Mie\\Programmi personali in Python\\Forecasting crypto with agents\\.venv\\Lib\\site-packages\\pandas\\core\\base.py\", line 245, in __getitem__\n",
      "    raise KeyError(f\"Column not found: {key}\")\n",
      "KeyError: 'Column not found: close'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 12-12 15:03:09] {732} WARNING - Model openai/gpt-oss-120b is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mData_Scraper\u001b[0m (to Admin):\n",
      "\n",
      "**Run the script below.** It automatically detects the timestamp and closing‑price columns, builds the 11:00 UTC → 10:59 UTC daily window, computes the max‑min variation, and writes the result to `data/processed/target_data.csv`.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from pathlib import Path\n",
      "import sys\n",
      "\n",
      "# ---------- 1️⃣ Load the raw CSV ----------\n",
      "raw_path = Path(\n",
      "    \"data/raw/Bitcoin futures (USDT) 25-03-2020-10-36-00_07-12-2025-00-00-00 timeframe 1m.csv\"\n",
      ")\n",
      "if not raw_path.is_file():\n",
      "    sys.exit(f\"❌ File not found: {raw_path}\")\n",
      "\n",
      "df = pd.read_csv(raw_path)\n",
      "\n",
      "# ---------- 2️⃣ Detect timestamp & close columns ----------\n",
      "# Helper to find a column name containing a keyword (case‑insensitive)\n",
      "def find_col(keywords):\n",
      "    for col in df.columns:\n",
      "        lowered = col.lower()\n",
      "        if any(k in lowered for k in keywords):\n",
      "            return col\n",
      "    return None\n",
      "\n",
      "ts_col = find_col([\"timestamp\", \"time\", \"date\"])\n",
      "close_col = find_col([\"close\", \"price\", \"last\"])\n",
      "\n",
      "if ts_col is None or close_col is None:\n",
      "    missing = []\n",
      "    if ts_col is None:\n",
      "        missing.append(\"timestamp column\")\n",
      "    if close_col is None:\n",
      "        missing.append(\"close/price column\")\n",
      "    sys.exit(f\"❌ Could not locate {', '.join(missing)} in the CSV. Columns present: {list(df.columns)}\")\n",
      "\n",
      "# Parse timestamps\n",
      "df[ts_col] = pd.to_datetime(df[ts_col], utc=True, errors=\"coerce\")\n",
      "if df[ts_col].isna().any():\n",
      "    sys.exit(\"❌ Some timestamps could not be parsed. Check the timestamp format.\")\n",
      "\n",
      "# ---------- 3️⃣ Build the custom day (starts at 11:00 UTC) ----------\n",
      "df[\"day\"] = (df[ts_col] - pd.Timedelta(hours=11)).dt.floor(\"D\")\n",
      "\n",
      "# ---------- 4️⃣ Compute max‑min close per custom day ----------\n",
      "target = (\n",
      "    df.groupby(\"day\")[close_col]\n",
      "    .agg(target_max_variation=lambda x: x.max() - x.min())\n",
      "    .reset_index()\n",
      ")\n",
      "\n",
      "# ---------- 5️⃣ Save the result ----------\n",
      "out_path = Path(\"data/processed/target_data.csv\")\n",
      "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
      "target.to_csv(out_path, index=False)\n",
      "\n",
      "print(f\"✅ Target file written to {out_path}\")\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAdmin\u001b[0m (to Data_Scraper):\n",
      "\n",
      "il target dev'essere la massima variazione di prezzo tra le 11 UTC e le 10:59 UTC del giorno successivo!!!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (afce36d5-3b67-4c7d-827e-ede0384ce611): Maximum number of consecutive auto-replies reached\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to Data_Scraper):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (c6d77bc3-089b-4335-b356-d48b33b23a68): Termination message condition on agent 'Data_Scraper' met\u001b[0m\n",
      "\n",
      "[FASE 3/5] Feature Engineering\n",
      "------------------------------------------------------------\n",
      "\u001b[33mAdmin\u001b[0m (to Data_Scraper):\n",
      "\n",
      "Load ALL CSV files from 'data/raw'. Clean NaN, synchronize timestamps. Merge with target_data.csv created previously. Create features: lags, moving averages, volatility, volume metrics. Save final dataset to data/processed/final_dataset.csv. Write EXECUTABLE Python code.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 12-12 15:21:41] {732} WARNING - Model openai/gpt-oss-120b is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mData_Scraper\u001b[0m (to Admin):\n",
      "\n",
      "Here’s a ready‑to‑run script that loads every CSV in `data/raw`, cleans missing values, aligns them on the `timestamp` column, merges with the existing `target_data.csv`, engineers lag, moving‑average, volatility and volume features, and writes the result to `data/processed/final_dataset.csv`.  \n",
      "\n",
      "```python\n",
      "import os, glob\n",
      "import pandas as pd\n",
      "from functools import reduce\n",
      "\n",
      "# 1️⃣ Load all raw CSVs, parse timestamps, clean NaNs\n",
      "raw_path = \"data/raw\"\n",
      "csv_files = glob.glob(os.path.join(raw_path, \"*.csv\"))\n",
      "dfs = []\n",
      "for f in csv_files:\n",
      "    df = pd.read_csv(f, parse_dates=[\"timestamp\"])\n",
      "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
      "    # Basic cleaning: drop rows where timestamp is missing, forward‑fill other NaNs\n",
      "    df = df.dropna(subset=[\"timestamp\"]).ffill().bfill()\n",
      "    dfs.append(df)\n",
      "\n",
      "# 2️⃣ Merge all raw tables on timestamp (outer join to keep every time point)\n",
      "merged_raw = reduce(lambda left, right: pd.merge(left, right, on=\"timestamp\", how=\"outer\"), dfs)\n",
      "merged_raw = merged_raw.sort_values(\"timestamp\").reset_index(drop=True)\n",
      "\n",
      "# 3️⃣ Load target data and merge (assume same timestamp column)\n",
      "target_path = \"data/processed/target_data.csv\"\n",
      "target_df = pd.read_csv(target_path, parse_dates=[\"timestamp\"])\n",
      "final_df = pd.merge(merged_raw, target_df, on=\"timestamp\", how=\"inner\")  # keep only times with target\n",
      "\n",
      "# 4️⃣ Feature engineering\n",
      "numeric_cols = final_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
      "numeric_cols.remove(\"timestamp\") if \"timestamp\" in numeric_cols else None\n",
      "\n",
      "# lag features (1‑3 steps)\n",
      "for col in numeric_cols:\n",
      "    for lag in range(1, 4):\n",
      "        final_df[f\"{col}_lag{lag}\"] = final_df[col].shift(lag)\n",
      "\n",
      "# moving averages and volatility (rolling windows 3 & 7)\n",
      "for col in numeric_cols:\n",
      "    final_df[f\"{col}_ma3\"] = final_df[col].rolling(3).mean()\n",
      "    final_df[f\"{col}_ma7\"] = final_df[col].rolling(7).mean()\n",
      "    final_df[f\"{col}_vol3\"] = final_df[col].rolling(3).std()\n",
      "    final_df[f\"{col}_vol7\"] = final_df[col].rolling(7).std()\n",
      "\n",
      "# volume‑type metrics: assume columns containing 'volume' are volume data\n",
      "vol_cols = [c for c in numeric_cols if \"volume\" in c.lower()]\n",
      "for col in vol_cols:\n",
      "    final_df[f\"{col}_sum3\"] = final_df[col].rolling(3).sum()\n",
      "    final_df[f\"{col}_sum7\"] = final_df[col].rolling(7).sum()\n",
      "\n",
      "# Drop rows that still contain NaNs after feature creation\n",
      "final_df = final_df.dropna().reset_index(drop=True)\n",
      "\n",
      "# 5️⃣ Save the processed dataset\n",
      "out_path = \"data/processed/final_dataset.csv\"\n",
      "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
      "final_df.to_csv(out_path, index=False)\n",
      "\n",
      "print(f\"Processed dataset saved to {out_path}, shape: {final_df.shape}\")\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Initiate Process and Save Log\n",
    "\n",
    "MAIN_TARGET_FILE = \"Bitcoin futures (USDT) 25-03-2020-10-36-00_07-12-2025-00-00-00 timeframe 1m.csv\"\n",
    "DATA_FOLDER = \"data/raw\" \n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ESECUZIONE SEQUENZIALE CON CONFERMA UTENTE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===== FASE 1: Setup Progetto =====\n",
    "print(\"\\n[FASE 1/5] Setup Progetto\")\n",
    "print(\"-\" * 60)\n",
    "phase1_result = admin.initiate_chat(\n",
    "    architect,\n",
    "    message=(\n",
    "        \"Create folders: src, data/raw, data/processed, models, testing. \"\n",
    "        \"Create src/config.py that loads GROQ_API_KEY with os.getenv(). \"\n",
    "        \"Write EXECUTABLE Python code in ```python blocks.\"\n",
    "    ),\n",
    "    # ⚠️ AUMENTATO max_turns per permettere conferma + esecuzione\n",
    "    max_turns=5\n",
    ")\n",
    "\n",
    "# ===== FASE 2: Creazione del TARGET =====\n",
    "print(\"\\n[FASE 2/5] Creazione Target Variable\")\n",
    "print(\"-\" * 60)\n",
    "phase2_result = admin.initiate_chat(\n",
    "    data_scraper,\n",
    "    message=(\n",
    "        f\"CRITICAL TASK: Create the target variable from '{MAIN_TARGET_FILE}'. \"\n",
    "        \"The target is: maximum absolute price change (for example +2000 USDT or -2500 USDT) from 11:00 UTC each day to 10:59 UTC of the day after. \"\n",
    "        \"For each day: \"\n",
    "        \"1. Calculate: max_change = max(close) - min(close) for that window \"\n",
    "        \"2. Store as 'target_max_variation' column \"\n",
    "        \"Save to data/processed/target_data.csv. \"\n",
    "        \"Write EXECUTABLE Python code.\"\n",
    "    ),\n",
    "    max_turns=5\n",
    ")\n",
    "\n",
    "# ===== FASE 3: Feature Engineering =====\n",
    "print(\"\\n[FASE 3/5] Feature Engineering\")\n",
    "print(\"-\" * 60)\n",
    "phase3_result = admin.initiate_chat(\n",
    "    data_scraper,\n",
    "    message=(\n",
    "        f\"Load ALL CSV files from '{DATA_FOLDER}'. \"\n",
    "        \"Clean NaN, synchronize timestamps. \"\n",
    "        \"Merge with target_data.csv created previously. \"\n",
    "        \"Create features: lags, moving averages, volatility, volume metrics. \"\n",
    "        \"Save final dataset to data/processed/final_dataset.csv. \"\n",
    "        \"Write EXECUTABLE Python code.\"\n",
    "    ),\n",
    "    max_turns=5\n",
    ")\n",
    "\n",
    "# ===== FASE 4: Model Training =====\n",
    "print(\"\\n[FASE 4/5] Model Training\")\n",
    "print(\"-\" * 60)\n",
    "phase4_result = admin.initiate_chat(\n",
    "    model_builder,\n",
    "    message=(\n",
    "        \"Load data/processed/final_dataset.csv. \"\n",
    "        \"Target column: 'target_max_variation'. \"\n",
    "        \"Train CPU-compatible model (RandomForest or XGBoost). \"\n",
    "        \"Use train_test_split. \"\n",
    "        \"Save trained model to models/best_model.pkl. \"\n",
    "        \"Write EXECUTABLE Python code.\"\n",
    "    ),\n",
    "    max_turns=5\n",
    ")\n",
    "\n",
    "# ===== FASE 5: Backtesting =====\n",
    "print(\"\\n[FASE 5/5] Backtesting & Optimization\")\n",
    "print(\"-\" * 60)\n",
    "phase5_result = admin.initiate_chat(\n",
    "    optimizer,\n",
    "    message=(\n",
    "        \"Load model from models/best_model.pkl. \"\n",
    "        \"Run backtest with liquidation logic: 100/L - 0.5%. \"\n",
    "        f\"Initial capital: {INITIAL_CAPITAL_USDT} USDT. \"\n",
    "        f\"Commission: {TRADING_COMMISSION_PCT}. \"\n",
    "        \"Optimize Leverage (1-100), SL, TP to maximize log_growth. \"\n",
    "        \"Save optimal parameters to models/optimal_params.json. \"\n",
    "        \"Write EXECUTABLE Python code.\"\n",
    "    ),\n",
    "    max_turns=5\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TUTTE LE FASI COMPLETATE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- Save Log ---\n",
    "save_chat_history_to_txt(manager, filename_prefix=\"BTC_Sequential_WithTarget\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
